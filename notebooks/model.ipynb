{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "All of the necessary function definitions and imports are done in this notebook, which is then imported to the other _presentation_ notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some standard import statements\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "import time\n",
    "import decimal\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.magic('load_ext autoreload')\n",
    "ipython.magic('autoreload 2')\n",
    "ipython.magic('matplotlib inline')\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The idea of this function is to load every data set into a dictionary. For every guide configuration, there are runs of each of the four pre-storage times. There are 7 configurations in total, so this yields 28 data sets. We also want to be able to access every run from each configuration, or every run from each pre-storage time. This amounts to 7 + 4 = 11 additional data sets (plus one more for the 'all', 'all' case). Rather than iteratively reload these every time, they're stored once in a dictionary for later access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-72951ebf11e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinear_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_fit' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "\n",
    "\n",
    "def load_all_data(start_point, normalize_flag):\n",
    "    \"\"\"A function to load data and sum counts for individual runs.\n",
    "    \n",
    "    Arguments:\n",
    "        start_point {string} -- A string to determine what start point will be\n",
    "            used as a reference for all time stamps. Options are:\n",
    "                'p_beam' - start at the first measurement from the proton \n",
    "                beam data\n",
    "        normalize_flag {boolean, optional} -- Flag to normalize the data \n",
    "            accroding to a series of routines dependent on run_type.\n",
    "    \n",
    "    Returns:\n",
    "        data_dict -- A dictionary of n x 5 data array of the results from \n",
    "            loading the run data. The number of rows corresponds to the number \n",
    "            of runs loaded. The five columns are:\n",
    "                0 - the run start time in seconds since the experimental start\n",
    "                1 - the storage time (0 if direct shot)\n",
    "                2 - the number of UCN counts\n",
    "                3 - sqrt(N) error in number of UCN counts\n",
    "                4 - [day].[run number] of measurement\n",
    "            The key pairs to be used are:\n",
    "                key 0: config {string} -- The options are:\n",
    "                'NORM' - normalization\n",
    "                'JPTI' - JP Ti guide with NiP\n",
    "                'JPSU' - JP SUS guide with NiP\n",
    "                'DISK' - SS Disk\n",
    "                'GD01' - UGD01 guide\n",
    "                'GD03' - UGD03 guide\n",
    "                'EPSU' - EP SUS guide with NiP\n",
    "                'all'  - all of the above\n",
    "                key 1: run_type {string} -- The options are:\n",
    "                'shot' - direct shot measurements \n",
    "                's005' - 5 second storage\n",
    "                's020' - 20 second storage\n",
    "                's100' - 100 second storage\n",
    "                'all'  - all of the above\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate an empty dictionary\n",
    "    data_dict = {}\n",
    "    \n",
    "    # instantiate the lists of configurations and run type\n",
    "    config_list   = ['NORM', 'JPTI', 'JPSU', 'DISK', 'GD01', 'GD03', 'EPSU']\n",
    "    run_type_list = ['shot', 's005', 's020', 's100']\n",
    "    \n",
    "    # iterate over configurations and run types\n",
    "    for config in config_list:\n",
    "        \n",
    "        for run_type in run_type_list:\n",
    "            \n",
    "            # do some stuff and save to dict\n",
    "            \n",
    "        # can do the 'config', 'all' case here\n",
    "    \n",
    "    # have to do the 'all', 'run_type' case here\n",
    "    for run_type in run_type_list\n",
    "    \n",
    "    return data_dict\n",
    "    \n",
    "#     # initialize an empty array to hold the loaded data\n",
    "#     data = np.zeros((1,5))\n",
    "    \n",
    "#     if (start_point == 'p_beam'):\n",
    "        \n",
    "#         first_run_time = get_first_p_beam_time()\n",
    "    \n",
    "#     # perform the source_normalization routine to retrive fit parameters\n",
    "#     if (normalize_flag):\n",
    "#         norm_parameters, norm_errors = source_normalization(run_type)\n",
    "\n",
    "#     # Every file in the directory containing main detector run data is iterated over.\n",
    "#     # Here the /sorted directory contains just runs deemed good for analysis.\n",
    "#     for filename in os.listdir('../data_main/sorted'):\n",
    "\n",
    "#         # Only the files matching our desired configuration and run type are \n",
    "#         # selected. The '.tof' condition is just so we don't perform the\n",
    "#         # analysis twice per run.\n",
    "#         if ((config in filename) and (run_type in filename) and \n",
    "#         ('.tof' in filename)):\n",
    "\n",
    "#             # open the text file associated with the run\n",
    "#             f = open( '../data_main/sorted/' + filename[0:22] + '.txt')  \n",
    "#             lines = f.readlines()\n",
    "#             f.close()\n",
    "            \n",
    "#             # grab the epoch time for run start\n",
    "#             date_time = filename[1:3].zfill(2) + '.12.2017 ' + lines[26][15:23]\n",
    "#             pattern = '%d.%m.%Y %H:%M:%S'\n",
    "#             run_start_time = int(time.mktime(time.strptime(date_time, pattern)))\n",
    "            \n",
    "#             # This function returns the start time of the very first run. \n",
    "#             run_start_time = run_start_time - first_run_time\n",
    "\n",
    "#             # if the run_type is shot, then storage time is set to 0\n",
    "#             if (run_type == \"shot\"):\n",
    "\n",
    "#                 storage_time = 0\n",
    "\n",
    "#             # otherwise grab the storage time from the run type\n",
    "#             else:\n",
    "\n",
    "#                 storage_time = int(run_type[1:4])\n",
    "\n",
    "#             # The data is retrieved from the .tof file\n",
    "#             count_data = np.loadtxt('../data_main/sorted/' + filename[0:22] + \n",
    "#             '.tof', usecols = (1))\n",
    "\n",
    "#             # specific data cut for run 35 on the 8th\n",
    "#             if ((filename[2:3] == '8') and (filename[10:12] == '35')):\n",
    "\n",
    "#                 counts = np.sum(count_data[150:1000])\n",
    "\n",
    "#             # specific data cut for run 66 on the 8th\n",
    "#             elif ((filename[2:3] == '8') and (filename[10:12] == '66')):\n",
    "\n",
    "#                 counts = np.sum(count_data[150:1500])\n",
    "\n",
    "#             # specific data cut for run 88 on the 8th\n",
    "#             elif ((filename[2:3] == '8') and (filename[10:12] == '88')):\n",
    "\n",
    "#                 counts = np.sum(count_data[150:2500])\n",
    "\n",
    "#             # cut the data normally\n",
    "#             else:\n",
    "\n",
    "#                 counts = np.sum(count_data[150:-1])\n",
    "\n",
    "#             # normalize the data depending on the normalize_flag\n",
    "#             # !!! Note that this normalization operatin is not taking into account the need\n",
    "#             # to correct based on absolute counts. i.e. here the direct shot has \n",
    "#             # a linear slope of ~-8, whereas the storage time measurements will be more \n",
    "#             # like -0.2. This is wrong, must be corrected.\n",
    "#             if (normalize_flag):\n",
    "#                 extrap_counts = source_fit(0, norm_parameters[0], \n",
    "#                                           norm_parameters[1])\n",
    "#                 interp_counts = source_fit(run_start_time, norm_parameters[0], \n",
    "#                                            norm_parameters[1])\n",
    "                \n",
    "                \n",
    "#                 norm_factor = extrap_counts / interp_counts\n",
    "                \n",
    "#                 counts = counts * norm_factor\n",
    "                \n",
    "\n",
    "#             # if this is the first file loaded, then assign the values to the data array, otherwise\n",
    "#             # append the vector of values to the existing array\n",
    "#             if (data[0,0] == 0):\n",
    "\n",
    "#                 data[0,0] = run_start_time\n",
    "#                 data[0,1] = storage_time\n",
    "#                 data[0,2] = counts\n",
    "#                 data[0,3] = np.sqrt(counts)\n",
    "\n",
    "#                 # saving the [day].[run number] can be useful for debugging\n",
    "#                 # requires enlarging the arrays\n",
    "#                 data[0,4] = int(filename[1:3]) + \\\n",
    "#                 (0.001 * int(filename[9:12]))\n",
    "\n",
    "#             # otherwise we make a vector and append it\n",
    "#             else:\n",
    "\n",
    "#                 run_data = np.zeros((1,5))\n",
    "#                 run_data[0,0] = run_start_time\n",
    "#                 run_data[0,1] = storage_time\n",
    "#                 run_data[0,2] = counts \n",
    "#                 run_data[0,3] = np.sqrt(counts)\n",
    "\n",
    "#                 # saving the [day].[run number] can be useful for debugging\n",
    "#                 # requires enlarging the arrays\n",
    "#                 run_data[0,4] = int(filename[1:3]) + \\\n",
    "#                 (0.001 * int(filename[9:12]))\n",
    "                \n",
    "#                 data = np.vstack((data, run_data))\n",
    "    \n",
    "#     # we return the data sorted by time\n",
    "#     return data[data[:,0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f84c75bac50f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "d['h'] = 1\n",
    "d.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
